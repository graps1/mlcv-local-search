{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from mlcv import greedy_search, generate_points, render_pcl, render_stats, distance_to_origin\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1) # for reproducability\n",
    "\n",
    "def cf_prime(data,combs):\n",
    "    dthr = 0.1\n",
    "    return distance_to_origin(data,combs) - dthr\n",
    "\n",
    "def cf(data,combs):\n",
    "    return np.zeros(combs.shape)\n",
    "\n",
    "def stop(idx, indexing, target_partition_count=0):\n",
    "    if idx < 100:\n",
    "        return False\n",
    "    nr_partitions = np.unique(indexing).shape[0]\n",
    "    return nr_partitions == target_partition_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs multiple instances of the greedy search algorithm on various datasets and stores the results in the `./experiments` subdirectory. For this matter, three files are created:\n",
    "\n",
    "* `meta.csv` stores the settings for each run, with the columns\n",
    "    * `didx`, which contains an index for the used dataset\n",
    "    * `partinit`, which is the number of random partitions the search algorithm is initialized with\n",
    "    * `N`, number of neighbours considered in each iteration. If `NaN` or `None`, then all neighbours are compared\n",
    "    * `M`, number of samples drawn in order to compute the reduced cost. If `NaN` or `None`, then the costs are computed explicitly\n",
    "    * `settingname`, name or short description for the search instance\n",
    "* `runs.csv` stores the run (i.e., every iteration) of each search instance, with the columns\n",
    "    * `run`, which links the iteration to the corresponding run\n",
    "    * `n_neighbours`, which stores the current number of considered neighbours\n",
    "    * `partcount`, which contains the current number of partitions\n",
    "    * `rc`, which stores the computed reduced costs\n",
    "    * `t_neighbours`, which stores the overall time required to compute all neighbours\n",
    "    * `t_rcs_mean`, which stores the mean time required to compute the reduced cost per neighbour\n",
    "    * `t_rcs_std`, which contains the respective standard deviation and\n",
    "    * `t_rcs_sum`, which contains the overall time required to compute every cost for every neighbour\n",
    "* `data.csv` stores the used datasets (i.e. a row for every point) with ground-truth and computed partitions\n",
    "    * `didx`, links every point to its corresponding dataset\n",
    "    * `x`, `y`, `z` contain the x, y and z-coordinate of every point\n",
    "    * `gt` contains the partition index of this point \n",
    "    * and columns for every considered run, which contain the computed partition index for this point\n",
    "    \n",
    "(overall execution time is approx. 1h, 10min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    generate_points(noise=[0.01]*3, distribution=[30 ]*3),\n",
    "    generate_points(noise=[0.01]*3, distribution=[80 ]*3),\n",
    "    generate_points(noise=[0.01]*3, distribution=[150]*3),\n",
    "    generate_points(noise=[0.01]*4, distribution=[250]*4),\n",
    "    generate_points(noise=[0.01]*5, distribution=[300]*5),\n",
    "]\n",
    "\n",
    "combs = [\n",
    "    # dataset, #intial_random_partitions, N, M, name_of_setting\n",
    "    [0, 1, 40,      1000, \"0-S-S\"],\n",
    "    [0, 1, None,    1000, \"0-E-S\"],\n",
    "    [0, 1, 40,      None, \"0-S-E\"],\n",
    "    [0, 1, None,    None, \"0-E-E\"],\n",
    "    \n",
    "    [1, 1, 40,      5000, \"1-S-S\"],\n",
    "    [1, 1, None,    5000, \"1-E-S\"],\n",
    "    [1, 1, 40,      None, \"1-S-E\"],\n",
    "    [1, 1, None,    None, \"1-E-E\"],\n",
    "    \n",
    "    [2, 1, 50,      5000, \"2-S-S\"],\n",
    "    [3, 1, 50,     10000, \"3-S-S\"],\n",
    "    [4, 1, 50,     15000, \"4-S-S\"],\n",
    "    \n",
    "    [2, 1, 50,       500, \"2-S-SL\"],\n",
    "    [2, 1,  5,      5000, \"2-SL-S\"],\n",
    "    \n",
    "    [2, 10, 50,     5000, \"2-S-S-IP10\"],\n",
    "    [2, 50, 50,     5000, \"2-S-S-IP50\"],\n",
    "    [2,100, 50,     5000, \"2-S-S-IP100\"]\n",
    "]\n",
    "\n",
    "dfmeta = pd.DataFrame(combs,columns=[\"didx\",\"partinit\",\"N\",\"M\",\"settingname\"])\n",
    "dfruns = pd.DataFrame()\n",
    "dfdata = pd.DataFrame(columns=[\"didx\",\"x\",\"y\",\"z\",\"gt\"]+[c[4] for c in combs])\n",
    "\n",
    "pbar = tqdm(combs, total=len(combs))\n",
    "already_stored = set()\n",
    "for didx, indexing_init, N, M, name in pbar:\n",
    "    data, ground_truth = dataset[didx]\n",
    "    \n",
    "    if didx not in already_stored:\n",
    "        tmp = pd.DataFrame()\n",
    "        tmp[\"x\"] = data[:,0]\n",
    "        tmp[\"y\"] = data[:,1]\n",
    "        tmp[\"z\"] = data[:,2]\n",
    "        tmp[\"didx\"] = didx\n",
    "        tmp[\"gt\"] = ground_truth\n",
    "        tmp[[c[4] for c in combs]] = None\n",
    "        dfdata = pd.concat((dfdata, tmp))\n",
    "        already_stored.add(didx)\n",
    "    \n",
    "    current_indexing = np.random.choice(indexing_init, data.shape[0])\n",
    "    partcount_gt = np.unique(ground_truth).shape[0]\n",
    "    stopping_criteria = lambda idx,indexing: stop(idx,indexing,target_partition_count=partcount_gt)\n",
    "    \n",
    "    alg = greedy_search(data, current_indexing, cf, cf_prime, stop=stopping_criteria, N=N, M=M)\n",
    "    for indexing, v, k, stats_bm in alg:\n",
    "        current_indexing = indexing\n",
    "        dfruns = dfruns.append({ **stats_bm, \"run\": name }, ignore_index=True)\n",
    "    \n",
    "    dfdata.loc[dfdata[\"didx\"] == didx, name] = current_indexing \n",
    "    \n",
    "    \n",
    "dfruns.to_csv(\"./experiments/runs.csv\", index=False)\n",
    "dfmeta.to_csv(\"./experiments/meta.csv\", index=False)\n",
    "dfdata.to_csv(\"./experiments/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit019ac1ddd4de423abf9b086ca7e6df0b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
